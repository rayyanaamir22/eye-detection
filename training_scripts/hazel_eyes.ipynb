{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI1DlRVYRfWl",
        "outputId": "537648c4-6d34-428b-f7fe-9e4d84258d18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.7-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.8.3)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.59.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.3)\n",
            "Downloading roboflow-1.2.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.0 pillow-avif-plugin-1.5.2 roboflow-1.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5tZ6lK1_N3mx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtP5cxGcQJSF",
        "outputId": "d76aa44a-33c4-48be-cdb5-1ba48a3169b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dataset downloaded to: /content/Eye-Color-1\n"
          ]
        }
      ],
      "source": [
        "# get dataset from roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"HTjMDkIgyNz0OGcJh7a9\")\n",
        "\n",
        "# Make sure project slug is correct\n",
        "project = rf.workspace(\"fashion-by4cb\").project(\"eye-color-bc6ji\")\n",
        "\n",
        "# Version 1 or 4 works (use 4 if you want the most recent)\n",
        "dataset = project.version(4).download(\"folder\")\n",
        "\n",
        "print(\"Dataset downloaded to:\", dataset.location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pTjyoh-Q8A5",
        "outputId": "49c92883-1d6a-44fe-8475-bca48f3b3d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 332 images belonging to 6 classes.\n",
            "Found 11 images belonging to 6 classes.\n",
            "Found 10 images belonging to 5 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 980ms/step - accuracy: 0.2203 - loss: 2.3284 - val_accuracy: 0.2727 - val_loss: 1.7897\n",
            "Epoch 2/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 866ms/step - accuracy: 0.4252 - loss: 1.4166 - val_accuracy: 0.3636 - val_loss: 1.9964\n",
            "Epoch 3/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 807ms/step - accuracy: 0.5469 - loss: 1.1469 - val_accuracy: 0.1818 - val_loss: 2.1250\n",
            "Epoch 4/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 975ms/step - accuracy: 0.6311 - loss: 0.8957 - val_accuracy: 0.2727 - val_loss: 1.7287\n",
            "Epoch 5/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 928ms/step - accuracy: 0.7136 - loss: 0.7345 - val_accuracy: 0.3636 - val_loss: 2.1378\n",
            "Epoch 6/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 761ms/step - accuracy: 0.8128 - loss: 0.5276 - val_accuracy: 0.3636 - val_loss: 2.1412\n",
            "Epoch 7/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 958ms/step - accuracy: 0.7782 - loss: 0.4662 - val_accuracy: 0.2727 - val_loss: 2.1224\n",
            "Epoch 8/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 944ms/step - accuracy: 0.9127 - loss: 0.3093 - val_accuracy: 0.3636 - val_loss: 2.8734\n",
            "Epoch 9/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 841ms/step - accuracy: 0.9634 - loss: 0.1802 - val_accuracy: 0.3636 - val_loss: 2.8689\n",
            "Epoch 10/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 808ms/step - accuracy: 0.9629 - loss: 0.1622 - val_accuracy: 0.4545 - val_loss: 2.8159\n",
            "Epoch 11/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 953ms/step - accuracy: 0.9592 - loss: 0.1637 - val_accuracy: 0.3636 - val_loss: 3.3547\n",
            "Epoch 12/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 938ms/step - accuracy: 0.9850 - loss: 0.1075 - val_accuracy: 0.3636 - val_loss: 3.0461\n",
            "Epoch 13/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 784ms/step - accuracy: 0.9926 - loss: 0.0584 - val_accuracy: 0.5455 - val_loss: 3.6158\n",
            "Epoch 14/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 931ms/step - accuracy: 1.0000 - loss: 0.0404 - val_accuracy: 0.5455 - val_loss: 3.2621\n",
            "Epoch 15/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 976ms/step - accuracy: 1.0000 - loss: 0.0232 - val_accuracy: 0.5455 - val_loss: 3.7578\n",
            "Epoch 16/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 962ms/step - accuracy: 1.0000 - loss: 0.0133 - val_accuracy: 0.4545 - val_loss: 4.0559\n",
            "Epoch 17/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 996ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 0.5455 - val_loss: 3.9502\n",
            "Epoch 18/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0066 - val_accuracy: 0.5455 - val_loss: 4.0274\n",
            "Epoch 19/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 992ms/step - accuracy: 1.0000 - loss: 0.0066 - val_accuracy: 0.4545 - val_loss: 4.3075\n",
            "Epoch 20/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 813ms/step - accuracy: 1.0000 - loss: 0.0054 - val_accuracy: 0.5455 - val_loss: 4.1916\n"
          ]
        }
      ],
      "source": [
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Data generators with rescaling\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen   = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen  = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Point to the actual dataset folders\n",
        "train_dir = \"Eye-Color-4/train\"\n",
        "val_dir   = \"Eye-Color-4/valid\"\n",
        "test_dir  = \"Eye-Color-4/test\"\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Build a small CNN\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=(*IMG_SIZE, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(train_gen.num_classes, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_gen, validation_data=val_gen, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D1qMRiHaqKd",
        "outputId": "4166f8ec-994c-4fc0-ede9-9d6e441eeb3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class mapping: {0: 'amber', 1: 'blue', 2: 'brown', 3: 'green', 4: 'grey', 5: 'hazel'}\n"
          ]
        }
      ],
      "source": [
        "# Map index → label for predictions later\n",
        "class_indices = train_gen.class_indices\n",
        "idx_to_class = {v:k for k,v in class_indices.items()}\n",
        "print(\"Class mapping:\", idx_to_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqxE8Lqoasen",
        "outputId": "ccd95548-0754-4963-ba05-5ac9dc6a3984"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Save for tfjs or later use\n",
        "model.save(\"eye_color_v0.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.10 64-bit ('3.10.10')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "94822f8b6a6c0cc023c9b69730703efa46d02f80b0a2fde3a36bb1e72ce59969"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
